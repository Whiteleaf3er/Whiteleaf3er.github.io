<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>大三下随笔</title>
    <url>/2020/03/23/%E5%A4%A7%E4%B8%89%E4%B8%8B%E9%9A%8F%E7%AC%94/</url>
    <content><![CDATA[<h2 id="理性的判断，这是一个转折点"><a href="#理性的判断，这是一个转折点" class="headerlink" title="理性的判断，这是一个转折点"></a>理性的判断，这是一个转折点</h2><p>2020年03月23日，来北京的第9天，明天正式开始实习了，而我还没有做出充分的准备，因为我还在想，我真的应该出来实习吗？</p>
<p>疫情条件，只身一人，这些其实定居下来都会适应，但是想想未来的不确定性，还是会有所焦虑 。由于大三上毛概的翻车和某些科考的不是很理想，成绩排名18%，算上美赛2.5加分看作裸分，7%（视独我加分），前13%拥有保研资格，但是目前不知道他人的加分情况，所以目前的状态就是标准的保研边缘人了。</p>
<p>再看特保，国赛加美赛可以获得特保的资格，但是还要看学院能否分到名额，也得看自己奖项、论文和均分的硬实力够不够，所以这也属于不确定性。</p>
<blockquote>
<p>发展的好，保研+特保两手准备，万无一失，大三下出去实习，丰富经验，为夏令营导师面试和大四实习面试做准备，一切井井有条，层层递进。</p>
</blockquote>
<blockquote>
<p>发展的不好，保研落马，特保无名额或竞争失败，8月初再开始准备考研，考研大概率失败，大三下的实习意义也不大。</p>
</blockquote>
<p>如果选择呆在实验室度过这半年，或许一切都解决了？</p>
<p>这是一个折中的方案。</p>
<blockquote>
<p> 在实验室划水，去混比赛，最好能再发一篇1作论文，再特保上增加自己的竞争力（前提是学院有名额，自己能有机会参与特保面试），让特保稳下来。更重要的是，划水过程中可以规划考研。</p>
</blockquote>
<blockquote>
<p>虽然没有实习经历，但是保研、特保、考研三手准备，基本不会翻车。</p>
</blockquote>
<h2 id="既来之，则安之"><a href="#既来之，则安之" class="headerlink" title="既来之，则安之"></a>既来之，则安之</h2><p>但是既然出来了，所去所为，我该如何规划呢？</p>
<ul>
<li>3.24——4.30 刷<strong>机试题</strong>、专业课、大学数学，准备<strong>夏令营资料</strong>（简历，推荐信，常问英文..），问py有没有比赛参赛差人的，混混<strong>比赛</strong>经历（特保），若可参加，至提交前尽量跟进</li>
<li>5.01——5.30 继续准备夏令营相关的（机试，专业课等），<strong>联系意向高校导师</strong></li>
<li>6.01——7.30 夏令营期限，<strong>参加夏令营</strong>，特别是和老师沟通好的</li>
<li>8.01——之后 此时实习离职，准备<strong>考研+夏令营/预推免</strong>，<strong>关注特保消息</strong></li>
</ul>
<p>所以，重点在于：</p>
<ul>
<li>前期认为自己能保上，<strong>积极准备保研</strong>夏令营，并且为特保也做一手准备，看是否能参加比赛</li>
<li>中期<strong>参加夏令营</strong>，争取拿到优秀营员</li>
<li>后期<strong>准备考研</strong>，若保研确定参加预推免，若保研落马积极准备考研，且关注特报消息</li>
</ul>
<p>当然，实习进度跟进也是很重要，这是一个实习生的任务</p>
<p>另外在此罗列一下目标夏令营院校：</p>
<p>华科、北航    并且在7月也联系一波本校的老师，获取一个名额</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>后端优化-CRF/MRF</title>
    <url>/2020/03/21/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96-CRF-MRF/</url>
    <content><![CDATA[<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>在语音分割获取数据标签时，是对原图进行人工的描绘出边框，然后再标注RGB的值的，这里就会有问题，在边缘处的<strong>标签很有可能是错误的</strong>，无法匹配其周围的像素标签。为了解决这种<strong>不连续性</strong>，我们可以用一种平滑的形式。我们需要确保目标占据图片中的连续区域，这样给定的像素和其周围像素的标签就是一样的。</p>
<p>下文就是为了解决这个问题，<strong>利用原始图像像素间的相似性来修正标签</strong></p>
<a id="more"></a>



<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><p>对于每个像素<code>i</code>具有类别标签<code>xi</code>还有对应的观测值<code>yi</code>，这样每个<strong>像素点作为节点</strong>，<strong>像素与像素间的关系作为边</strong>，即构成了一个条件随机场。而且我们通过观测变量yi来推测像素i对应的类别标签xi。条件随机场如下： </p>
<p><img src="http://img.blog.csdn.net/20160904201421377" alt=""></p>
<p>yi：卷积后的值（观测值）</p>
<p>xi：类别标签<br><strong>条件随机场符合吉布斯分布。</strong></p>
<p>在全链接的 CRF 模型中，有一个对应的能量函数：<br><img src="https://img-blog.csdn.net/20170205171536559?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>那么E(x)由两个部分组成，可以简单理解为：</p>
<blockquote>
<p>E(x)=一元函数+二元函数</p>
</blockquote>
<p>一元函数：来自于前端<strong>FCN的输出</strong><br>二元函数：是描述<strong>像素点与像素点之间的关系</strong>，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。所以这样CRF能够使图片尽量在边界处分割。<br><strong>全连接条件随机场的不同就在于，二元势函数描述的是每一个像素与其他所有像素的关系，所以叫“全连接”。</strong></p>
<p>通过对这个能量函数优化求解，把明显不符合事实识别判断剔除，替换成合理的解释，得到对FCN的图像语义预测结果的优化，生成最终的语义分割结果。 </p>
<h2 id="MRF"><a href="#MRF" class="headerlink" title="MRF"></a>MRF</h2><p>在Deep Parsing Network中使用的是MRF（马尔可夫随机场），它的公式具体的定义和CRF类似，只不过作者对二元势函数进行了修改：<br><img src="https://img-blog.csdn.net/20170205173057349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>其中，作者加入λk为label context，因为只是定义了<strong>两个像素同时出现的频率</strong>，而λk可以对一些情况进行惩罚。<br>比如，人可能在桌子旁边，但是在桌子下面的可能性就更小一些。所以这个量可以<strong>学习不同情况出现的概率</strong>。<br>而原来的距离d(i,j)只定义了两个像素间的关系，作者在这儿加入了个triple penalty，即还引入了j附近的z，这样描述三方关系便于得到<strong>更充足的局部上下文</strong>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>随机场的出现是为了弥补标签提取的不足，并且可以获得更丰富的上下文信息，而上下文信息、局部信息在语义分割中是很重要的，在PspNet中也提到了邻近信息、局部信息的重要性，这些信息可以更好的进行判断预测。</p>
]]></content>
      <categories>
        <category>滴滴实习-语义分割</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
        <tag>CRF</tag>
        <tag>MRF</tag>
      </tags>
  </entry>
  <entry>
    <title>图像语义分割经典结构</title>
    <url>/2020/03/20/%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%8F%E5%85%B8%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h2 id="文章简介"><a href="#文章简介" class="headerlink" title="文章简介"></a>文章简介</h2><p>在<a href="http://nanhua.space/2020/03/17/语义分割综述/#more" target="_blank" rel="noopener">语义分割综述</a>里其实已经介绍了，但是，那篇文章介绍比较详细，也贴了一些代码，统筹总局不够清晰明了，所以决定再理一下。</p>
<p>当然，可以预见的是，这个方向的内容自己还没入门，属于探索阶段的整理，所以之后肯定还会有带有自我实践理解的总结性的这样一篇综述的</p>
<h2 id="语义分割Idea"><a href="#语义分割Idea" class="headerlink" title="语义分割Idea"></a>语义分割Idea</h2><p>传统的图像识别分类网络最后往往是通过全连接层shape为1维即<strong>1 x Nclasses</strong>的向量再进行图像分类工作。语义分割又称密集预测，像素级别的预测，那么最后的<strong>输出一定是H’*W’ x Nclasses</strong>，也就是每个点都进行了各自的预测工作。</p>
<a id="more"></a>

<ul>
<li>那么一个想法就油然而生了，<strong>全连接层替换掉</strong>不就可以了吗？</li>
<li>但还有需要考虑的，通常卷积过后图像分辨率降低或保持不变（取决于步长），通道数增加，在卷积层的输出后改变通道数容易，也使用卷积就可以达到目的，<strong>分辨率如何恢复</strong>到原图或较大分辨率的状态呢？也就是如何进行上采样呢？</li>
<li>继续考虑，如果上采样问题解决了，这样利用到的仅仅是多次卷积后的最后输出特征，较深层的特征，<strong>浅层的特征图</strong>能不能利用起来呢？</li>
</ul>
<p>这三个点其实很容易想到，那么我们从第一个点直接进入图像语义分割领域开山之作——FCN</p>
<h2 id="FCN（CVPR2015）"><a href="#FCN（CVPR2015）" class="headerlink" title="FCN（CVPR2015）"></a>FCN（CVPR2015）</h2><p>第一篇论文提出的FCN其实就已经考虑到了我们想到的3个idea，话不多说，直接看FCN的三个主要特征：</p>
<h4 id="卷积替代全连接"><a href="#卷积替代全连接" class="headerlink" title="卷积替代全连接"></a>卷积替代全连接</h4><p>如图所示，像 <code>VGG16</code> 分类网络的全连接层（<code>fc6</code>，<code>fc7</code>）被转换为全卷积层。生成了一个<strong>低分辨率</strong>的类的热图</p>
<p><img src="https://pic3.zhimg.com/80/42d85c5f7ddcb3f527666b250f62f5d6_1440w.png" alt=""></p>
<p>通过这一步，Encoder成型</p>
<h4 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h4><p>然后使用经双线性插值初始化的<strong>反卷积</strong>，<strong>提高图像分辨率</strong></p>
<h4 id="融合多层特征"><a href="#融合多层特征" class="headerlink" title="融合多层特征"></a>融合多层特征</h4><p>在上采样的每一个阶段通过<strong>融合（简单地相加） <code>VGG16</code> 中的低层</strong>（<code>conv4</code>和<code>conv3</code>）的更加粗糙但是分辨率更高的特征图进一步<strong>细化特征</strong>，恢复在降低分辨率时损失的特征。</p>
<p>在这里可以讨论一下<strong>池化</strong>，在Encoder时，池化增加感受野，降低分辨率，但也使得空间信息丢失（比如maxpool损失了部分背景信息），带步长的卷积同理。所以在Decoder使如何恢复这一部分信息是有必要的</p>
<img src="https://pic2.zhimg.com/80/ccb6dd0a7f207134ae7690974c3e88a5_1440w.png" style="zoom: 80%;" />

<p>看似3的idea都得到了处理，但有两点问题是很直接明显的</p>
<ul>
<li>上采样采取反卷积的形式，计算量消耗大</li>
<li>特征融合有没有更有效的方式，也就是Decoder部分如何有效改进</li>
</ul>
<h2 id="Segnet（TPAMI-2015）"><a href="#Segnet（TPAMI-2015）" class="headerlink" title="Segnet（TPAMI 2015）"></a>Segnet（TPAMI 2015）</h2><p>Segnet是典型的Encoder-Decoder结构，其特殊之处主要在于<strong>上采样的方式</strong>（提取特征主要针对高层特征，其实这里还没有FCN做的好，没有融合低层特征）</p>
<p><img src="https://pic1.zhimg.com/80/6cab0e3643d16ccab0a1bf1909813484_1440w.png" alt="img"></p>
<h4 id="反池化上采样"><a href="#反池化上采样" class="headerlink" title="反池化上采样"></a>反池化上采样</h4><p>细节就是记录了池化时的位置信息，更好的保持高频信息的完整性，但<strong>也会忽略邻近的信息</strong>。</p>
<p>一个很大的优势：参数量减少，不需要学习</p>
<p><img src="https://img-blog.csdnimg.cn/20190325194741517.png" alt="img"></p>
<p>问题：</p>
<ul>
<li>池化损失的邻近信息如何恢复？</li>
<li>需要融合低特征吗？（其实融合低层特征也会恢复部分池化中损失的信息，因为低层特征还没被池化233）</li>
</ul>
<h2 id="Unet（MICCAI-2015）"><a href="#Unet（MICCAI-2015）" class="headerlink" title="Unet（MICCAI 2015）"></a>Unet（MICCAI 2015）</h2><p>一个很明显的trick，将<strong>Encoder</strong>的特征图<strong>拼接</strong>至每个阶段<strong>Decoder</strong>的上采样特征图。来自网络中较早层的跳跃连接（在下采样操作之前）<strong>提供必要的细节</strong>，以便为分割边界重建精确的形。这一个trick基本就对Segnet的问题提出了一个解决方案。</p>
<p>还有一个细节是上采用采用的<strong>转置卷积（反卷积）</strong>，而不是Segnet提出的反池化（为什么呢？反池化不是参数量更少吗？猜测一下可能是因为实验结果中反卷积效果更好吧233）</p>
<p><img src="https://img-blog.csdnimg.cn/20190325194852702.png" alt="img"></p>
<p>FCN，Segnet，Unet，三种网络已经提出了许多trick来解决语义分割的问题，特别是针对以下问题</p>
<ul>
<li>编码时stride=2的Conv操作或池化操作后图像分辨率减少<strong>损失的信息如何恢复</strong>，特别是池化操作的邻近信息的恢复——反池化</li>
<li>上采用如何减少参数计算量——反池化（当然看具体情况决定反池化还是反卷积）</li>
<li>如何保留低层信息——融合拼接</li>
</ul>
<p>之后的语义分割网络结构创新点在哪呢？</p>
<h2 id="DeepLab-v1（ICLR-2015）"><a href="#DeepLab-v1（ICLR-2015）" class="headerlink" title="DeepLab v1（ICLR 2015）"></a>DeepLab v1（ICLR 2015）</h2><p>deeplabv1的设计亮点：</p>
<ul>
<li>采用了空洞卷积。利用空洞卷积在不增加参数的情况下扩大了感受野的范围。</li>
<li>CRF（条件随机场）的处理。CRF的后期处理可以恢复边界信息，更加准确的定位，更好的提升语义分割的准确率。</li>
</ul>
<p>这里要指出一个细节，也是为什么这里要引入空洞卷积：</p>
<p>在FCN时有一个操作：为了防止输出的分辨率太小，作者对原图直接padding，但是后果是噪声的引入。所以要解决的问题是采取某种方式使<strong>分辨率不那么小</strong>，但还要考虑的地方是<strong>便于fine-tune</strong>，也就是网络结构和感受野的固定</p>
<p>很简单的想法是减少池化层，但是这样改变结构就不能fine-tune了。DeepLab v1首先的想法是将池化的stride调整为1，这样自然便达到了分辨率保证的效果，但是改变stride后边层的<strong>感受野发生了变化</strong>。</p>
<p>进而才提出了空洞卷积。如下图所示，（a）和（c）即为<strong>相同的感受野</strong>，Atrous Convolution（空洞卷积）使得模型感受野不变，从而可以fine-tune，保证输出结果的精细化。</p>
<p><img src="https://pic3.zhimg.com/80/766fc04b86b72f7e09d8f8ff6cb648e2_1440w.png" alt=""></p>
<h2 id="DeepLab-v2（TPAMI-2017）"><a href="#DeepLab-v2（TPAMI-2017）" class="headerlink" title="DeepLab v2（TPAMI 2017）"></a>DeepLab v2（TPAMI 2017）</h2><p>deeplabv2在v1的基础之上增加了ASPP(空洞空间金字塔池化)模块。如下图所示，通过<strong>不同尺度的空洞率（rate）</strong>来提取不同尺寸的特征，更好的融合不同的特征，达到更好的分割效果。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190723/58dbd09b149e4579988651425e0576a1.jpeg" alt="img"></p>
<h2 id="DeepLab-v3（2017）"><a href="#DeepLab-v3（2017）" class="headerlink" title="DeepLab v3（2017）"></a>DeepLab v3（2017）</h2><p>deeplabv3的创新点主要在于<strong>串行-并行</strong>的结构，如下图所示。以v2中的不同rate的空洞卷积叠加串行结构，<strong>中间某一层的输出Low-Level-Features直接传入解码器</strong>，而串行结构后的<strong>最终输出进入并行结构采用不同rate的空洞卷积/1x1卷积/pooling进行特征提取</strong>，拼接后上采用再去和Low-Level-Features进行<strong>拼接</strong>进行后续预测等工作。</p>
<p><img src="https://img-blog.csdnimg.cn/20191111203311239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<h2 id="PSPNet（CVPR-2017）"><a href="#PSPNet（CVPR-2017）" class="headerlink" title="PSPNet（CVPR 2017）"></a>PSPNet（CVPR 2017）</h2><p>特色在于<strong>金字塔池化模块</strong>，对Encoder最后输出的特征进行不同尺度的池化，因为池化后会resize，所以分辨率统一可以拼接融合，之后再进行卷积调整通道数、预测等操作。</p>
<p>需要注意，PSPNet 通过<strong>引入空洞卷积来修改基础的 ResNet 架构</strong>，特征经过最初的池化，在整个编码器网络中以相同的分辨率进行处理（原始图像输入的<code>1/4</code>），直到它到达空间池化模块。</p>
<p>还需要知道PSP的目的：<strong>融合图像的上下文信息，注重像素之间的关联性</strong>。PSPnet利用预训练模型提取特征后，将采用金字塔池化模块提取图像的上下文信息，并将上下文信息与提取的特征进行堆叠后，经过上采样得到最终的输出。也就是车在路上跑，鱼在水里游，车和鱼分不开，看水和路也许就分开了。</p>
<p><img src="https://img-blog.csdnimg.cn/20190402191049975.png" alt="img"></p>
<h2 id="RefineNet（CVPR2017）"><a href="#RefineNet（CVPR2017）" class="headerlink" title="RefineNet（CVPR2017）"></a>RefineNet（CVPR2017）</h2><p>RefineNet，一个通用的多路径优化网络，<strong>利用了整个下采样过程中可用的所有信息</strong>，使用远程残差连接实现高分辨率的预测。通过这种方式，可以使用<strong>早期卷积中的细粒度特征来直接细化捕捉高级语义特征的更深的网络层</strong>。RefineNet 的各个组件使用遵循恒等映射思想的残差连接，这允许网络进行有效的端到端训练。（意思也就是在Encoder时的所有信息都尽量用了，并且是利用早期特征来细化深层特征）</p>
<p><img src="https://img-blog.csdnimg.cn/20190402191317912.png" alt="img"></p>
<p>从上图可以看出，在Encoder（使用的Resnet）得到了从低层到高层的各个特征图。再利用RefineNet模块进行融合操作。RefineNet如下图</p>
<p><img src="https://img-blog.csdnimg.cn/20190402191340916.png" alt="img"></p>
<ul>
<li>RCU：该部分用来作为一个自适应卷积集，主要是微调ResNet的权重。</li>
<li>MRF：将所有的输入特征图的<strong>分辨率调整</strong>为最大特征图的分辨率尺寸。</li>
<li>CRP：通过链式残差池化部分，可以有效的捕捉上下文信息。</li>
<li>输出卷积：在最终的输出之前，再加一个RCU。</li>
</ul>
<p>RefineNet还没有研究源码，理解不深刻，还请见谅</p>
<p>下一篇文章更新后端操作，比如CRF</p>
]]></content>
      <categories>
        <category>滴滴实习-语义分割</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
        <tag>Deeplab</tag>
        <tag>Segnet</tag>
        <tag>Unet</tag>
        <tag>Pspnet</tag>
        <tag>FCN</tag>
        <tag>RefineNet</tag>
      </tags>
  </entry>
  <entry>
    <title>语义分割实例-DeeplabV3-MobilenetV2</title>
    <url>/2020/03/19/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%AE%9E%E4%BE%8B-DeeplabV3-MobilenetV2/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇文章主要通过<strong>DeeplabV3-MobilenetV2对VOC21</strong>分类数据集进行语义分割</p>
<p>网络结构可参考上一篇综述中的Deeplab部分，此篇文章主要介绍通用的数据集状况以及训练流程</p>
<a id="more"></a>

<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h4 id="VOC数据集"><a href="#VOC数据集" class="headerlink" title="VOC数据集"></a>VOC数据集</h4><p>包括背景一共21个类别</p>
<p>dataset2文件夹中包括3个文件：</p>
<blockquote>
<p>dataset2 </p>
<blockquote>
<p>jpg -原图，未经处理，分辨率大小不统一</p>
</blockquote>
<blockquote>
<p>png - 打了标签后的图像，分辨率500 x 375 ， 依然不满足分辨率要求</p>
</blockquote>
<blockquote>
<p>train_data.txt</p>
</blockquote>
</blockquote>
<p>train-data.txt文件如图，存储的是原图和标签的文件名</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319164202932.png" alt="image-20200319164202932"></p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>两个问题</p>
<p>第一个是分辨率不同，如何统一到需要的输入512 x 512</p>
<p>第二个是数据不经处理模型鲁棒性较低，如何使原始数据所含特征更加丰富</p>
<p>对于第一个问题，很容易想到直接resize((w,h))，但是问题出现了，直接拉伸收缩，图像会出现扭曲和失真，所以还需要点技巧是填补，在上下或左右填补黑色，来使最终分辨率统一</p>
<p>比如原图分辨率为256 x 128，通过第四行得到scale = 2，nw x nh = 512 x 256，先resize到nw x nh的状态，此时虽然不满足512 x 512，但是W已经满足了，只不过H高度矮了点，重点是没有失真，那么将矮的少的那一部分用黑色填充即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterbox_image</span><span class="params">(image, size, type)</span>:</span></span><br><span class="line">    iw, ih = image.size</span><br><span class="line">    w, h = size</span><br><span class="line">    scale = min(w/iw, h/ih)</span><br><span class="line">    nw = int(iw*scale)</span><br><span class="line">    nh = int(ih*scale)</span><br><span class="line">    </span><br><span class="line">    image = image.resize((nw,nh), Image.NEAREST)</span><br><span class="line">    <span class="keyword">if</span>(type==<span class="string">"jpg"</span>):</span><br><span class="line">        new_image = Image.new(<span class="string">'RGB'</span>, size, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">elif</span>(type==<span class="string">"png"</span>):</span><br><span class="line">        new_image = Image.new(<span class="string">'RGB'</span>, size, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    new_image.paste(image, ((w-nw)//<span class="number">2</span>, (h-nh)//<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> new_image,nw,nh</span><br></pre></td></tr></table></figure>

<p>第13行：后两个参数表示贴上去贴在哪——所贴位置的左上角</p>
<p>效果图是这样的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319170205592.png" alt="image-20200319170205592"></p>
<p>接下来解决第二个问题，答案是数据增强，也就是增加一些噪声，比如颜色变，翻转，缩放等等</p>
<p>看看效果图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319170242538.png" alt="image-20200319170242538"></p>
<p>前16行通过双三次插值(Image.BICUBIC)调整到随机选定的缩放比例H x W</p>
<p>17-25行随机贴到全黑的图中</p>
<p>26-31行随机翻转图像，左右翻转即镜像</p>
<p>最后在rgb转化为hsv，随机加噪声造成图像扭曲</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_data</span><span class="params">(image, label, input_shape, jitter=<span class="number">.2</span>, hue=<span class="number">.2</span>, sat=<span class="number">1.1</span>, val=<span class="number">1.1</span>)</span>:</span></span><br><span class="line">    h, w = input_shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># resize image</span></span><br><span class="line">    rand_jit1 = rand(<span class="number">1</span>-jitter,<span class="number">1</span>+jitter) <span class="comment">#0.8-1.2</span></span><br><span class="line">    rand_jit2 = rand(<span class="number">1</span>-jitter,<span class="number">1</span>+jitter)</span><br><span class="line">    new_ar = w/h * rand_jit1/rand_jit2</span><br><span class="line">    scale = rand(<span class="number">.6</span>, <span class="number">1.4</span>)</span><br><span class="line">    <span class="keyword">if</span> new_ar &lt; <span class="number">1</span>:</span><br><span class="line">        nh = int(scale*h)</span><br><span class="line">        nw = int(nh*new_ar)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nw = int(scale*w)</span><br><span class="line">        nh = int(nw/new_ar)</span><br><span class="line">    image = image.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">    label = label.resize((nw,nh), Image.BICUBIC)</span><br><span class="line">    <span class="comment"># place image</span></span><br><span class="line">    dx = int(rand(<span class="number">0</span>, w-nw))</span><br><span class="line">    dy = int(rand(<span class="number">0</span>, h-nh))</span><br><span class="line">    new_image = Image.new(<span class="string">'RGB'</span>, (w,h), (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    new_label = Image.new(<span class="string">'RGB'</span>, (w,h), (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)) <span class="comment">#生成全黑的图</span></span><br><span class="line">    new_image.paste(image, (dx, dy)) <span class="comment"># 随机贴上去</span></span><br><span class="line">    new_label.paste(label, (dx, dy))</span><br><span class="line">    image = new_image</span><br><span class="line">    label = new_label</span><br><span class="line">    <span class="comment"># flip image or not</span></span><br><span class="line">    flip = rand()&lt;<span class="number">.5</span></span><br><span class="line">    <span class="keyword">if</span> flip: </span><br><span class="line">        image = image.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line">        label = label.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># distort image</span></span><br><span class="line">    hue = rand(-hue, hue)</span><br><span class="line">    sat = rand(<span class="number">1</span>, sat) <span class="keyword">if</span> rand()&lt;<span class="number">.5</span> <span class="keyword">else</span> <span class="number">1</span>/rand(<span class="number">1</span>, sat)</span><br><span class="line">    val = rand(<span class="number">1</span>, val) <span class="keyword">if</span> rand()&lt;<span class="number">.5</span> <span class="keyword">else</span> <span class="number">1</span>/rand(<span class="number">1</span>, val)</span><br><span class="line">    x = rgb_to_hsv(np.array(image)/<span class="number">255.</span>)</span><br><span class="line">    x[..., <span class="number">0</span>] += hue</span><br><span class="line">    x[..., <span class="number">0</span>][x[..., <span class="number">0</span>]&gt;<span class="number">1</span>] -= <span class="number">1</span></span><br><span class="line">    x[..., <span class="number">0</span>][x[..., <span class="number">0</span>]&lt;<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line">    x[..., <span class="number">1</span>] *= sat</span><br><span class="line">    x[..., <span class="number">2</span>] *= val</span><br><span class="line">    x[x&gt;<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    x[x&lt;<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    image_data = hsv_to_rgb(x)</span><br><span class="line">    <span class="keyword">return</span> image_data,label</span><br></pre></td></tr></table></figure>

<h2 id="训练-train-py"><a href="#训练-train-py" class="headerlink" title="训练-train.py"></a>训练-train.py</h2><ol>
<li><p>获取model ， model = Deeplabv3()</p>
</li>
<li><p>获取权重参数 </p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">weights_path = get<span class="constructor">_file()</span></span><br><span class="line">model.load<span class="constructor">_weights(<span class="params">weight_path</span>)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>初步获取数据集，打开txt文件</p>
<p>with open() as f:</p>
<p>​    lines = f.readlines()</p>
</li>
<li><p>打乱行，划分训练集和验证集</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">np</span><span class="selector-class">.random</span><span class="selector-class">.seed</span>(10101)</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">np</span><span class="selector-class">.random</span><span class="selector-class">.shuffle</span>(<span class="selector-tag">lines</span>)</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="5">
<li><p>保存CKPT的方式，隔几代保存一次</p>
<p>checkpoint_period = ModelCheckpoint()</p>
</li>
<li><p>学习率下降的方式，要考虑到前期大学习率，后期小学习率兼顾效率和效果，什么时候下降，一种方式是val_loss两次不下降就降低学习率来更细致的Train<br>reduce_lr = ReduceLROnPlateau()</p>
</li>
<li><p>早停,一种情况是val_loss几次不下降就说明训练OK了<br>early_stopping = EarlyStopping()</p>
</li>
<li><p>损失函数定义<br>K.binary_crossentropy()</p>
</li>
<li><p>优化器，其实compile时有一个参数就需要指定优化器</p>
   <figure class="highlight nix"><table><tr><td class="code"><pre><span class="line">model.compile(<span class="attr">loss</span> = loss,</span><br><span class="line">      <span class="attr">optimizer</span> = Adam(<span class="attr">lr=1e-3),</span></span><br><span class="line">      <span class="attr">metrics</span> = ['accuracy'])</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="10">
<li><p>开始训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit_generator(generate_arrays_from_file(lines[:num_train], batch_size),</span><br><span class="line">        steps_per_epoch=max(<span class="number">1</span>, num_train//batch_size),</span><br><span class="line">        validation_data=generate_arrays_from_file(lines[num_train:], batch_size),</span><br><span class="line">        validation_steps=max(<span class="number">1</span>, num_val//batch_size),</span><br><span class="line">        epochs=<span class="number">30</span>,</span><br><span class="line">        initial_epoch=<span class="number">0</span>,</span><br><span class="line">        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br></pre></td></tr></table></figure>
</li>
<li><p>保存权重h5文件</p>
<p>model.save_weights（）</p>
</li>
</ol>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>其实可以发现，通过model（）封装后，语义分割模型和图像分类主干代码基本一致，训练流程也无所差距</p>
]]></content>
      <categories>
        <category>滴滴实习-语义分割</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
        <tag>Deeplab</tag>
        <tag>Mobilenet</tag>
      </tags>
  </entry>
  <entry>
    <title>Segmentation综述</title>
    <url>/2020/03/17/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="什么是语义分割"><a href="#什么是语义分割" class="headerlink" title="什么是语义分割"></a>什么是语义分割</h1><p>计算机视觉的相关任务从图像识别分类到目标检测到语义分割，任务量的难度逐步加大，图像识别仅仅需要预测出一张图片是哪一个类别，目标检测需要将图片中包含的各个类都用方框框起来，对每个框进行预测，而语义分割是对每一个像素点进行预测，通常又被称为密集预测。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/20200319175328741.png" alt="image-20200319175328741"></p>
<a id="more"></a>

<h1 id="语义分割模型"><a href="#语义分割模型" class="headerlink" title="语义分割模型"></a>语义分割模型</h1><p>通常图像分割模型是<strong>Encoder-Decoder</strong>结构。Encoder部分通过<strong>下采样</strong>降低输入的空间分辨率，从而生成一个低分辨率的特征映射（计算高效且能够有效区分不同类别）；Decoder则对这些特征描述进行<strong>上采样</strong>，将其恢复成全分辨率的分割图。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319175510110.png" alt="image-20200319175510110"></p>
<h2 id="Segnet模型"><a href="#Segnet模型" class="headerlink" title="Segnet模型"></a>Segnet模型</h2><h4 id="简述Segnet"><a href="#简述Segnet" class="headerlink" title="简述Segnet"></a>简述Segnet</h4><p>segnet属于基础的语义分割模型，结构类似于上图，编码部分可以采用不同的网络结构进行下采样降低分辨率并提取特征，比如VGG或Mobilenet，VGG再经典不过，都比较熟悉，3*3的卷积和池化堆叠，这里简述一下Mobilenet的网络结构</p>
<h4 id="Encoder模块——Mobilenet网络结构"><a href="#Encoder模块——Mobilenet网络结构" class="headerlink" title="Encoder模块——Mobilenet网络结构"></a>Encoder模块——Mobilenet网络结构</h4><p>MobileNet模型是Google针对手机等嵌入式设备提出的一种轻量级的深层神经网络，其使用的核心思想便是<strong>depthwise separable convolution</strong>。</p>
<p>对于一个卷积过程而言：</p>
<p>正常的卷积操作：假设有一个3×3大小的卷积层，其输入通道为16、输出通道为32。我是这样理解的，WxHx16的图像通过3x3x16的卷积核进行卷积，卷积过程中每个3x3x1遍历对应的WxHx1，如果是SAME的形式输出分辨率不变则是，WxHx1，而3x3x16就会得到16个WxHx1即WxHx16，WxH中各个像素点相加（16个相叠加），则得到了WxHx1，但是这里不仅是3x3x16，还要注意输出通道数，也就是3x3x16x32，32个3x3x16的卷积核会遍历输入图像中的16个输入通道，所以输出是WxHx32，所需参数为16×32×3×3=4608个。</p>
<img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319175805198.png" alt="image-20200319175805198" style="zoom:80%;" />

<p>应用深度可分离卷积：第一步为<strong>Depthwise conv</strong>用16个3×3大小的卷积核<strong>分别遍历</strong>16通道的数据，得到了16个特征图谱。也就是第i个3x3的卷积核负责遍历第i个输入通道得到WxH的特征图。接着第二步为<strong>Pointwise conv</strong>，用32个1×1大小的卷积核遍历这16个特征图，其实第二步也就是正常的卷积操作。所需参数为16×3×3+16×32×1×1=656个。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319180458654.png" alt="image-20200319180458654"></p>
<p>重点在于理解<strong>Depthwise conv</strong>中的每个卷积核负责一个输入通道输出一个通道，而不是正常卷积操作中的每个卷积核负责一个通道，但输出的是1/32个输出通道（还有其他卷积核的结果加起来，才是一个输出通道）</p>
<p>知道了核心模块其余就很简单了，各种卷积的堆叠</p>
<p><img src="https://img-blog.csdnimg.cn/20191030153845940.png#pic_center" alt="在这里插入图片描述"></p>
<h4 id="Encoder代码（Mobilenet结构）"><a href="#Encoder代码（Mobilenet结构）" class="headerlink" title="Encoder代码（Mobilenet结构）"></a>Encoder代码（Mobilenet结构）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">IMAGE_ORDERING = <span class="string">'channels_last'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu6</span><span class="params">(x)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> K.relu(x, max_value=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_conv_block</span><span class="params">(inputs, filters, alpha, kernel=<span class="params">(<span class="number">3</span>, <span class="number">3</span>)</span>, strides=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">	channel_axis = <span class="number">1</span> <span class="keyword">if</span> IMAGE_ORDERING == <span class="string">'channels_first'</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">	filters = int(filters * alpha)</span><br><span class="line">	x = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>), name=<span class="string">'conv1_pad'</span>, data_format=IMAGE_ORDERING  )(inputs)</span><br><span class="line">	x = Conv2D(filters, kernel , data_format=IMAGE_ORDERING  ,</span><br><span class="line">										padding=<span class="string">'valid'</span>,</span><br><span class="line">										use_bias=<span class="literal">False</span>,</span><br><span class="line">										strides=strides,</span><br><span class="line">										name=<span class="string">'conv1'</span>)(x)</span><br><span class="line">	x = BatchNormalization(axis=channel_axis, name=<span class="string">'conv1_bn'</span>)(x)</span><br><span class="line">	<span class="keyword">return</span> Activation(relu6, name=<span class="string">'conv1_relu'</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_depthwise_conv_block</span><span class="params">(inputs, pointwise_conv_filters, alpha,</span></span></span><br><span class="line"><span class="function"><span class="params">													depth_multiplier=<span class="number">1</span>, strides=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>, block_id=<span class="number">1</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">	channel_axis = <span class="number">1</span> <span class="keyword">if</span> IMAGE_ORDERING == <span class="string">'channels_first'</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">	pointwise_conv_filters = int(pointwise_conv_filters * alpha)</span><br><span class="line"></span><br><span class="line">	x = ZeroPadding2D((<span class="number">1</span>, <span class="number">1</span>) , data_format=IMAGE_ORDERING , name=<span class="string">'conv_pad_%d'</span> % block_id)(inputs)</span><br><span class="line">	x = DepthwiseConv2D((<span class="number">3</span>, <span class="number">3</span>) , data_format=IMAGE_ORDERING ,</span><br><span class="line">														 padding=<span class="string">'valid'</span>,</span><br><span class="line">														 depth_multiplier=depth_multiplier,</span><br><span class="line">														 strides=strides,</span><br><span class="line">														 use_bias=<span class="literal">False</span>,</span><br><span class="line">														 name=<span class="string">'conv_dw_%d'</span> % block_id)(x)</span><br><span class="line">	x = BatchNormalization(</span><br><span class="line">			axis=channel_axis, name=<span class="string">'conv_dw_%d_bn'</span> % block_id)(x)</span><br><span class="line">	x = Activation(relu6, name=<span class="string">'conv_dw_%d_relu'</span> % block_id)(x)</span><br><span class="line"></span><br><span class="line">	x = Conv2D(pointwise_conv_filters, (<span class="number">1</span>, <span class="number">1</span>), data_format=IMAGE_ORDERING ,</span><br><span class="line">										padding=<span class="string">'same'</span>,</span><br><span class="line">										use_bias=<span class="literal">False</span>,</span><br><span class="line">										strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">										name=<span class="string">'conv_pw_%d'</span> % block_id)(x)</span><br><span class="line">	x = BatchNormalization(axis=channel_axis,</span><br><span class="line">																name=<span class="string">'conv_pw_%d_bn'</span> % block_id)(x)</span><br><span class="line">	<span class="keyword">return</span> Activation(relu6, name=<span class="string">'conv_pw_%d_relu'</span> % block_id)(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mobilenet_encoder</span><span class="params">( input_height=<span class="number">224</span> ,  input_width=<span class="number">224</span> , pretrained=<span class="string">'imagenet'</span> )</span>:</span></span><br><span class="line"></span><br><span class="line">	alpha=<span class="number">1.0</span></span><br><span class="line">	depth_multiplier=<span class="number">1</span></span><br><span class="line">	dropout=<span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	img_input = Input(shape=(input_height,input_width , <span class="number">3</span> ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = _conv_block(img_input, <span class="number">32</span>, alpha, strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">64</span>, alpha, depth_multiplier, block_id=<span class="number">1</span>) </span><br><span class="line">	f1 = x</span><br><span class="line"></span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">128</span>, alpha, depth_multiplier,</span><br><span class="line">														strides=(<span class="number">2</span>, <span class="number">2</span>), block_id=<span class="number">2</span>)  </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">128</span>, alpha, depth_multiplier, block_id=<span class="number">3</span>) </span><br><span class="line">	f2 = x</span><br><span class="line"></span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">256</span>, alpha, depth_multiplier,</span><br><span class="line">														strides=(<span class="number">2</span>, <span class="number">2</span>), block_id=<span class="number">4</span>)  </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">256</span>, alpha, depth_multiplier, block_id=<span class="number">5</span>) </span><br><span class="line">	f3 = x</span><br><span class="line"></span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">512</span>, alpha, depth_multiplier,</span><br><span class="line">														strides=(<span class="number">2</span>, <span class="number">2</span>), block_id=<span class="number">6</span>) </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">512</span>, alpha, depth_multiplier, block_id=<span class="number">7</span>) </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">512</span>, alpha, depth_multiplier, block_id=<span class="number">8</span>) </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">512</span>, alpha, depth_multiplier, block_id=<span class="number">9</span>) </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">512</span>, alpha, depth_multiplier, block_id=<span class="number">10</span>) </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">512</span>, alpha, depth_multiplier, block_id=<span class="number">11</span>) </span><br><span class="line">	f4 = x </span><br><span class="line"></span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">1024</span>, alpha, depth_multiplier,</span><br><span class="line">														strides=(<span class="number">2</span>, <span class="number">2</span>), block_id=<span class="number">12</span>)  </span><br><span class="line">	x = _depthwise_conv_block(x, <span class="number">1024</span>, alpha, depth_multiplier, block_id=<span class="number">13</span>) </span><br><span class="line">	f5 = x </span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> img_input , [f1 , f2 , f3 , f4 , f5 ]</span><br></pre></td></tr></table></figure>

<p>由上代码发现返回了$ f_i $（i=12345），这个是什么呢？</p>
<p>可以发现每个 $ f_i $到$ f_{i+1} $之间都有一次strides = （2，2）的conv操作，也就每次图像的分辨率都缩小了一倍</p>
<p>例如f4的shape是N x H/16 x W/16 x 512</p>
<p>而这些$ f_i $有什么用了，进入<strong>decoder</strong>部分吧</p>
<h4 id="Decoder模块"><a href="#Decoder模块" class="headerlink" title="Decoder模块"></a>Decoder模块</h4><p>Decoder将图像<strong>分辨率进行恢复</strong>，把<strong>获得的特征</strong>重新映射到图中的每一个像素点，用于<strong>每一个像素点的分类</strong>。</p>
<p>所以这也是为什么要保存$ f_i $的原因了，因为需要把特征进行再利用</p>
<p>如何将分辨率恢复呢？重点在于上采样模块</p>
<p><img src="https://img-blog.csdnimg.cn/20181223184533975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc1ODEw,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>具体的操作其实也就是逐行逐列一一复制添加</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> UpSampling2D</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>])</span><br><span class="line">x=x.reshape(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">x=tf.convert_to_tensor(x)</span><br><span class="line">y=UpSampling2D(size=(<span class="number">2</span>,<span class="number">2</span>))(x)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(y.eval())</span><br></pre></td></tr></table></figure>

<p>print(x):</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200319180645144.png" alt="image-20200319180645144"></p>
<p>print(y.eval()):</p>
<p><img src="E:%5CHexo%5Csource_posts%5Cupload%5Cimage-20200319180807838.png" alt="image-20200319180807838"></p>
<h4 id="Decoder代码"><a href="#Decoder代码" class="headerlink" title="Decoder代码"></a>Decoder代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> nets.mobilenet <span class="keyword">import</span> get_mobilenet_encoder</span><br><span class="line"></span><br><span class="line">IMAGE_ORDERING = <span class="string">'channels_last'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segnet_decoder</span><span class="params">(  f , n_classes , n_up=<span class="number">3</span> )</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> n_up &gt;= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">	o = f</span><br><span class="line">	o = ( ZeroPadding2D( (<span class="number">1</span>,<span class="number">1</span>) , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>, data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line">	<span class="comment"># 进行一次UpSampling2D，此时hw变为原来的1/8</span></span><br><span class="line">	<span class="comment"># 52,52,512</span></span><br><span class="line">	o = ( UpSampling2D( (<span class="number">2</span>,<span class="number">2</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	o = ( ZeroPadding2D( (<span class="number">1</span>,<span class="number">1</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	o = ( Conv2D( <span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>, data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 进行一次UpSampling2D，此时hw变为原来的1/4</span></span><br><span class="line">	<span class="comment"># 104,104,256</span></span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> range(n_up<span class="number">-2</span>):</span><br><span class="line">		o = ( UpSampling2D((<span class="number">2</span>,<span class="number">2</span>)  , data_format=IMAGE_ORDERING ) )(o)</span><br><span class="line">		o = ( ZeroPadding2D((<span class="number">1</span>,<span class="number">1</span>) , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">		o = ( Conv2D( <span class="number">128</span> , (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span> , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">		o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 进行一次UpSampling2D，此时hw变为原来的1/2</span></span><br><span class="line">	<span class="comment"># 208,208,128</span></span><br><span class="line">	o = ( UpSampling2D((<span class="number">2</span>,<span class="number">2</span>)  , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( ZeroPadding2D((<span class="number">1</span>,<span class="number">1</span>)  , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( Conv2D( <span class="number">64</span> , (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>  , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 此时输出为h_input/2,w_input/2,nclasses</span></span><br><span class="line">	o =  Conv2D( n_classes , (<span class="number">3</span>, <span class="number">3</span>) , padding=<span class="string">'same'</span>, data_format=IMAGE_ORDERING )( o )</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> o </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_segnet</span><span class="params">( n_classes , encoder  ,  input_height=<span class="number">416</span>, input_width=<span class="number">608</span> , encoder_level=<span class="number">3</span>)</span>:</span></span><br><span class="line">	<span class="comment"># encoder通过主干网络</span></span><br><span class="line">	img_input , levels = encoder( input_height=input_height ,  input_width=input_width )</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取hw压缩四次后的结果</span></span><br><span class="line">	feat = levels[encoder_level]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将特征传入segnet网络</span></span><br><span class="line">	o = segnet_decoder(feat, n_classes, n_up=<span class="number">3</span> )</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将结果进行reshape</span></span><br><span class="line">	o = Reshape((int(input_height/<span class="number">2</span>)*int(input_width/<span class="number">2</span>), <span class="number">-1</span>))(o)</span><br><span class="line">	o = Softmax()(o)</span><br><span class="line">	model = Model(img_input,o)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_segnet</span><span class="params">( n_classes ,  input_height=<span class="number">224</span>, input_width=<span class="number">224</span> , encoder_level=<span class="number">3</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">	model = _segnet( n_classes , get_mobilenet_encoder ,  input_height=input_height, input_width=input_width , encoder_level=encoder_level)</span><br><span class="line">	model.model_name = <span class="string">"mobilenet_segnet"</span></span><br><span class="line">	<span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>第49行：o = segnet_decoder(feat, n_classes, n_up=3 )</p>
<p>将分辨率降低4次后提取到的特征传入segnet网络进行上采样恢复，恢复后为H/2 x W/2 x nclasses，之后reshape为 HxW/4 x nclasses，再调用softmax函数进行分类</p>
<p>这里回想一下在一个图像5分类的工作中，某个batch_size = 1的batch，也就是一张图片在进入最后的softmax前shape为1 x 5，之后通过softmax得到5个类的概率值</p>
<p>而这里的segnet，<strong>1张图片通过softmax得到HxW/4 x nclasses的数据结构，表示的也就是H/2 x W/2这样一张图片，每个像素点都拥有对5个类别的预测概率值，也就实现了最初想要达到的对每个像素点进行预测</strong></p>
<h4 id="Segnet小结"><a href="#Segnet小结" class="headerlink" title="Segnet小结"></a>Segnet小结</h4><p>总的来说，Segnet是一个经典的语义分割网络结构，首先降低分辨率提取特征，再将某一次（第4次）提取到的特征进行上采样恢复分辨率，最后对大分辨率图像中每个像素点都进行类别预测</p>
<h2 id="Unet模型"><a href="#Unet模型" class="headerlink" title="Unet模型"></a>Unet模型</h2><h4 id="简述Unet"><a href="#简述Unet" class="headerlink" title="简述Unet"></a>简述Unet</h4><p>在进行segnet的详解的时候知道，其中<strong>只选了一个压缩了四次的特征层</strong>进行三次上采样得到最后的结果。<br>但是unet不一样，其<strong>用到了压缩了二、三、四次的特征层</strong>，最后输出图像分割的结果（可以选择是否需要压缩了一次的特征层）。也就是它利用了多个特征层，使得提取到的特征更加的丰富，为什么没有第一层呢？我猜想是因为第一层特征丰富度不够，所以直接放弃了2333</p>
<h4 id="Encoder模块"><a href="#Encoder模块" class="headerlink" title="Encoder模块"></a>Encoder模块</h4><p>与上述Segnet相同，采取Mobilenet来提取特征</p>
<h4 id="Decoder模块-1"><a href="#Decoder模块-1" class="headerlink" title="Decoder模块"></a>Decoder模块</h4><p>首先我们获得了f1，f2，f3，f4，f5，5个层次的特征，我们先看一下各自的shape</p>
<p>f1：208 x 208 x 64</p>
<p>f2：104 x 104 x 128</p>
<p>f3：52 x 52 x 256</p>
<p>f4：26 x 26 x 512</p>
<p>f5：13 x 13 x 1024</p>
<p>那么如何不同于Segnet仅利用到了f4，Unet如何利用f2-f4呢，其实主要就一个操作：</p>
<p>将fi上采样后达到与fi-1相同的分辨率，再进行concat拼接起来，再重复</p>
<p>我们来看一下代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> nets.mobilenet <span class="keyword">import</span> get_mobilenet_encoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IMAGE_ORDERING = <span class="string">'channels_last'</span></span><br><span class="line">MERGE_AXIS = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_unet</span><span class="params">( n_classes , encoder , l1_skip_conn=True,  input_height=<span class="number">416</span>, input_width=<span class="number">608</span>  )</span>:</span></span><br><span class="line"></span><br><span class="line">	img_input , levels = encoder( input_height=input_height ,  input_width=input_width )</span><br><span class="line">	[f1 , f2 , f3 , f4 , f5 ] = levels </span><br><span class="line"></span><br><span class="line">	o = f4</span><br><span class="line">	<span class="comment"># 26,26,512</span></span><br><span class="line">	o = ( ZeroPadding2D( (<span class="number">1</span>,<span class="number">1</span>) , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>, data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 52,52,512</span></span><br><span class="line">	o = ( UpSampling2D( (<span class="number">2</span>,<span class="number">2</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	<span class="comment"># 52,52,768</span></span><br><span class="line">	o = ( concatenate([ o ,f3],axis=MERGE_AXIS )  )</span><br><span class="line">	o = ( ZeroPadding2D( (<span class="number">1</span>,<span class="number">1</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	<span class="comment"># 52,52,256</span></span><br><span class="line">	o = ( Conv2D( <span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>, data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 104,104,256</span></span><br><span class="line">	o = ( UpSampling2D( (<span class="number">2</span>,<span class="number">2</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	<span class="comment"># 104,104,384</span></span><br><span class="line">	o = ( concatenate([o,f2],axis=MERGE_AXIS ) )</span><br><span class="line">	o = ( ZeroPadding2D((<span class="number">1</span>,<span class="number">1</span>) , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	<span class="comment"># 104,104,128</span></span><br><span class="line">	o = ( Conv2D( <span class="number">128</span> , (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span> , data_format=IMAGE_ORDERING ) )(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line">	<span class="comment"># 208,208,128</span></span><br><span class="line">	o = ( UpSampling2D( (<span class="number">2</span>,<span class="number">2</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> l1_skip_conn:</span><br><span class="line">		o = ( concatenate([o,f1],axis=MERGE_AXIS ) )</span><br><span class="line"></span><br><span class="line">	o = ( ZeroPadding2D((<span class="number">1</span>,<span class="number">1</span>)  , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( Conv2D( <span class="number">64</span> , (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>  , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">	o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line">	o =  Conv2D( n_classes , (<span class="number">3</span>, <span class="number">3</span>) , padding=<span class="string">'same'</span>, data_format=IMAGE_ORDERING )( o )</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 将结果进行reshape</span></span><br><span class="line">	o = Reshape((int(input_height/<span class="number">2</span>)*int(input_width/<span class="number">2</span>), <span class="number">-1</span>))(o)</span><br><span class="line">	o = Softmax()(o)</span><br><span class="line">	model = Model(img_input,o)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_unet</span><span class="params">( n_classes ,  input_height=<span class="number">224</span>, input_width=<span class="number">224</span> , encoder_level=<span class="number">3</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">	model =  _unet( n_classes , get_mobilenet_encoder ,  input_height=input_height, input_width=input_width  )</span><br><span class="line">	model.model_name = <span class="string">"mobilenet_unet"</span></span><br><span class="line">	<span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>我们来看看其中的15-24行做了什么：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">o = f4</span><br><span class="line"><span class="comment"># 26,26,512</span></span><br><span class="line">o = ( ZeroPadding2D( (<span class="number">1</span>,<span class="number">1</span>) , data_format=IMAGE_ORDERING ))(o)</span><br><span class="line">o = ( Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'valid'</span>, data_format=IMAGE_ORDERING))(o)</span><br><span class="line">o = ( BatchNormalization())(o)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 52,52,512</span></span><br><span class="line">o = ( UpSampling2D( (<span class="number">2</span>,<span class="number">2</span>), data_format=IMAGE_ORDERING))(o)</span><br><span class="line"><span class="comment"># 52,52,768</span></span><br><span class="line">o = ( concatenate([ o ,f3],axis=MERGE_AXIS )  )</span><br></pre></td></tr></table></figure>

<p>其实也就是对f4进行了一次<strong>上采样</strong>，26 x 26 x 512——52 x 52 x 512，此时分辨率与f3相同，再与f3进行<strong>拼接</strong>，f3为52 x 52 x 256，最后得到的shape为52 x 52 x 768</p>
<p>之后就是重复这个工作达到了<strong>利用多个特征层的目的</strong></p>
<h4 id="Unet小结"><a href="#Unet小结" class="headerlink" title="Unet小结"></a>Unet小结</h4><p>Unet相比于Segnet最大的改进之处即在于Decoder时<strong>利用了多个Encoder的特征层</strong>，核心在于将fi进行上采样后，与fi-1进行拼接，再重复工作</p>
<h2 id="Pspnet"><a href="#Pspnet" class="headerlink" title="Pspnet"></a>Pspnet</h2><h4 id="简述-Pspnet"><a href="#简述-Pspnet" class="headerlink" title="简述 Pspnet"></a>简述 Pspnet</h4><p>pspnet名字源于其主要采用了<strong>pspblock</strong></p>
<p>也就是psp模块。<br>psp模块的样式如下，其psp的核心重点是采用了步长不同，pool_size不同的平均池化层进行池化，然后将池化的结果重新resize到一个hw上后，再concatenate。<br>即：<br>红色：这是在每个特征map上执行全局平均池的最粗略层次，用于生成单个输出。<br>橙色：这是第二层，将特征map划分为2×2个子区域，然后对每个子区域进行平均池化。<br>蓝色：这是第三层，将特征 map划分为3×3个子区域，然后对每个子区域进行平均池化。<br>绿色：这是将特征map划分为6×6个子区域的最细层次，然后对每个子区域执行池化。<br><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200317221518265.png" alt=""></p>
<p>比如下图，在psp_block中，input的shape为18 x 18 x 1024</p>
<p>首先是红色的，也就是全局均值池化，使用的pool_size和stride为18 x 18，输出为1x 1x 1024，再通过1 x 1的卷积核进行通道数调整，再通过resize_image为18 x 18的分辨率，输出为18 x 18 x 512</p>
<p>接着是橙色的，首先将18 x 18 x 1024 划分为4个区域，再对么区域进行均值池化，其代码实现也就是使用pool_size和stride为9 x 9，此时输出为2 x 2 x 1024，再调整通道数和分辨率，保证输出也为18 x 18 x 512</p>
<p>后两种类似</p>
<p><img src="https://cdn.jsdelivr.net/gh/Whiteleaf3er/FigureBed/pictures/image-20200317222347479.png" alt=""></p>
<h4 id="Encoder模块-1"><a href="#Encoder模块-1" class="headerlink" title="Encoder模块"></a>Encoder模块</h4><p>类似，提取出多个特征层</p>
<h4 id="Decoder模块-2"><a href="#Decoder模块-2" class="headerlink" title="Decoder模块"></a>Decoder模块</h4><p>先上代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> nets.mobilenet <span class="keyword">import</span> get_mobilenet_encoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras_segmentation.models.model_utils <span class="keyword">import</span> get_segmentation_model</span><br><span class="line"></span><br><span class="line">IMAGE_ORDERING = <span class="string">'channels_last'</span></span><br><span class="line">MERGE_AXIS = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resize_image</span><span class="params">( inp ,  s , data_format )</span>:</span></span><br><span class="line">	<span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> Lambda( </span><br><span class="line">		<span class="keyword">lambda</span> x: tf.image.resize_images(</span><br><span class="line">			x , ( K.int_shape(x)[<span class="number">1</span>]*s[<span class="number">0</span>] ,K.int_shape(x)[<span class="number">2</span>]*s[<span class="number">1</span>] ))  </span><br><span class="line">		)( inp )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_block</span><span class="params">( feats , pool_factor )</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> IMAGE_ORDERING == <span class="string">'channels_first'</span>:</span><br><span class="line">		h = K.int_shape( feats )[<span class="number">2</span>]</span><br><span class="line">		w = K.int_shape( feats )[<span class="number">3</span>]</span><br><span class="line">	<span class="keyword">elif</span> IMAGE_ORDERING == <span class="string">'channels_last'</span>:</span><br><span class="line">		h = K.int_shape( feats )[<span class="number">1</span>]</span><br><span class="line">		w = K.int_shape( feats )[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># strides = [18,18],[9,9],[6,6],[3,3]</span></span><br><span class="line">	pool_size = strides = [int(np.round( float(h) /  pool_factor)), int(np.round(  float(w )/  pool_factor))]</span><br><span class="line"> </span><br><span class="line">	<span class="comment"># 进行不同程度的平均</span></span><br><span class="line">	x = AveragePooling2D(pool_size , data_format=IMAGE_ORDERING , strides=strides, padding=<span class="string">'same'</span>)( feats )</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 进行卷积</span></span><br><span class="line">	x = Conv2D(<span class="number">512</span>, (<span class="number">1</span> ,<span class="number">1</span> ), data_format=IMAGE_ORDERING , padding=<span class="string">'same'</span> , use_bias=<span class="literal">False</span> )( x )</span><br><span class="line">	x = BatchNormalization()(x)</span><br><span class="line">	x = Activation(<span class="string">'relu'</span> )(x)</span><br><span class="line"></span><br><span class="line">	x = resize_image( x , strides , data_format=IMAGE_ORDERING ) </span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_pspnet</span><span class="params">( n_classes , encoder ,  input_height=<span class="number">384</span>, input_width=<span class="number">576</span>  )</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> input_height%<span class="number">192</span> == <span class="number">0</span></span><br><span class="line">	<span class="keyword">assert</span> input_width%<span class="number">192</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	img_input , levels = encoder( input_height=input_height,input_width=input_width)</span><br><span class="line">	[f1 , f2 , f3 , f4 , f5 ] = levels </span><br><span class="line"></span><br><span class="line">	o = f5</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 对f5进行不同程度的池化</span></span><br><span class="line">	pool_factors = [ <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>]</span><br><span class="line">	pool_outs = [o ]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> p <span class="keyword">in</span> pool_factors:</span><br><span class="line">		pooled = pool_block(  o , p  )</span><br><span class="line">		pool_outs.append( pooled )</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 连接</span></span><br><span class="line">	o = Concatenate( axis=MERGE_AXIS)(pool_outs )</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 卷积</span></span><br><span class="line">	o = Conv2D(<span class="number">512</span>, (<span class="number">1</span>,<span class="number">1</span>), data_format=IMAGE_ORDERING, use_bias=<span class="literal">False</span> )(o)</span><br><span class="line">	o = BatchNormalization()(o)</span><br><span class="line">	o = Activation(<span class="string">'relu'</span> )(o)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 此时输出为[144,144,nclasses]</span></span><br><span class="line">	o = Conv2D( n_classes,(<span class="number">3</span>,<span class="number">3</span>),data_format=IMAGE_ORDERING, padding=<span class="string">'same'</span> )(o)</span><br><span class="line">	o = resize_image(o,(<span class="number">8</span>,<span class="number">8</span>),data_format=IMAGE_ORDERING)</span><br><span class="line">	o = Reshape((<span class="number">-1</span>,n_classes))(o)</span><br><span class="line">	o = Softmax()(o)</span><br><span class="line">	model = Model(img_input,o)</span><br><span class="line">	<span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_pspnet</span><span class="params">( n_classes ,  input_height=<span class="number">224</span>, input_width=<span class="number">224</span> )</span>:</span></span><br><span class="line"></span><br><span class="line">	model =  _pspnet( n_classes , get_mobilenet_encoder ,  input_height=input_height, input_width=input_width  )</span><br><span class="line">	model.model_name = <span class="string">"mobilenet_pspnet"</span></span><br><span class="line">	<span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>需要注意的是<strong>Pspnet中输入图像为576 x 576，不同于Segnet和Unet的416 x 416</strong></p>
<p>所以提取出的f5为 18 x 18 x 1024，也就是上边所举的例子</p>
<ul>
<li>通过不同程度的池化，每一种池化输出为18 x 18 x512</li>
<li><strong>4种池化的结果加上自己本身f5进行拼接</strong>，输出为 18 x 18 x 2560</li>
<li>再通过卷积调整通道数为512，输出为 18 x 18 x 512</li>
<li>再通过卷积<strong>调整通道数</strong>为nclasses准备softmax计算概率</li>
<li>18 x 18太小 ，还需要<strong>恢复分辨率大小，使用resize调整分辨率大小</strong>，即8倍的宽高</li>
<li>之后就是经典的softmax预测工作</li>
</ul>
<h4 id="Pspnet小结"><a href="#Pspnet小结" class="headerlink" title="Pspnet小结"></a>Pspnet小结</h4><p>重点在于核心结构Psp_block，<strong>对Encoder后的特征层进行不同程度的池化，再拼接</strong>，再恢复调整分辨率与通道数ge</p>
<h2 id="DeeplabV3-based-MobilenetV2"><a href="#DeeplabV3-based-MobilenetV2" class="headerlink" title="DeeplabV3 based MobilenetV2"></a>DeeplabV3 based MobilenetV2</h2><h4 id="简述MobilenetV2"><a href="#简述MobilenetV2" class="headerlink" title="简述MobilenetV2"></a>简述MobilenetV2</h4><p>MobileNet模型是一种轻量级的深层神经网络，其使用的核心思想便是<strong>depthwise separable convolution</strong>。</p>
<p>MobileNetV2是MobileNet的升级版，它具有两个特征点：</p>
<p>1、Inverted residuals，在ResNet50里的一个结构，bottleneck design结构，在3x3网络结构前利用1x1卷积降维，在3x3网络结构后，利用1x1卷积升维，相比直接使用3x3网络卷积效果更好，参数更少，先进行压缩，再进行扩张。而在MobileNetV2网络部分，其采用<strong>Inverted residuals结构</strong>，<strong>在3x3网络结构前利用1x1卷积升维，在3x3网络结构后，利用1x1卷积降维</strong>，先进行扩张，再进行压缩。</p>
<p>2、Linear bottlenecks，为了避免Relu对特征的破坏，在在3x3网络结构前利用1x1卷积升维，在3x3网络结构后，再利用1x1卷积降维后，<strong>不再进行Relu6层</strong>，直接进行残差网络的加法。</p>
<p>对比图是这样的</p>
<ul>
<li>MobileNet V1用<strong>DepthwiseConv和Conv进行堆叠</strong></li>
<li>ResNet 50用1 x 1卷积核<strong>降</strong>维再标准<strong>卷积</strong>再1 x 1卷积<strong>升</strong>维，并且会有一个<strong>short cut</strong></li>
<li>MobileNet V2用1 x 1卷积<strong>升</strong>维度再使用<strong>DepthwiseConv</strong>再<strong>降</strong>维，和Resnet相同，第二个1 x 1卷积后<strong>没有使用Relu函数</strong>，保证特征不被破坏，并且依然会有<strong>short cut</strong>结构</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/201911131056046.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<p>整体网络结构如下图所示，核心也就是刚才所说的<strong>bottleneck</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191101094224444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<p>bottleneck的代码如下所示，参数中block_id是指这种反残差结构的编号，当id=0时，expansion = 1，也就是<strong>第一个残差模块没有进行通道数的扩张</strong>，没有进行刚才所说的使用1 x 1卷积进行升维的过程</p>
<p>其他也如上所述，<strong>将Resnet 50的结构反过来，先升维，然后进行的不是普通的3 x 3 Conv，而是Depthwise Conv，再1 x 1降维，降维后没有使用Relu，最后进行拼接起来</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_inverted_res_block</span><span class="params">(inputs, expansion, stride, alpha, filters, block_id)</span>:</span></span><br><span class="line">    in_channels = backend.int_shape(inputs)[<span class="number">-1</span>]</span><br><span class="line">    pointwise_conv_filters = int(filters * alpha)</span><br><span class="line">    pointwise_filters = _make_divisible(pointwise_conv_filters, <span class="number">8</span>)</span><br><span class="line">    x = inputs</span><br><span class="line">    prefix = <span class="string">'block_&#123;&#125;_'</span>.format(block_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># part1 数据扩张</span></span><br><span class="line">    <span class="keyword">if</span> block_id:</span><br><span class="line">        <span class="comment"># Expand</span></span><br><span class="line">        x = Conv2D(expansion * in_channels,</span><br><span class="line">                          kernel_size=<span class="number">1</span>,</span><br><span class="line">                          padding=<span class="string">'same'</span>,</span><br><span class="line">                          use_bias=<span class="literal">False</span>,</span><br><span class="line">                          activation=<span class="literal">None</span>,</span><br><span class="line">                          name=prefix + <span class="string">'expand'</span>)(x)</span><br><span class="line">        x = BatchNormalization(epsilon=<span class="number">1e-3</span>,</span><br><span class="line">                                      momentum=<span class="number">0.999</span>,</span><br><span class="line">                                      name=prefix + <span class="string">'expand_BN'</span>)(x)</span><br><span class="line">        x = Activation(relu6, name=prefix + <span class="string">'expand_relu'</span>)(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prefix = <span class="string">'expanded_conv_'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> stride == <span class="number">2</span>:</span><br><span class="line">        x = ZeroPadding2D(padding=correct_pad(x, <span class="number">3</span>),</span><br><span class="line">                                 name=prefix + <span class="string">'pad'</span>)(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># part2 可分离卷积</span></span><br><span class="line">    x = DepthwiseConv2D(kernel_size=<span class="number">3</span>,</span><br><span class="line">                               strides=stride,</span><br><span class="line">                               activation=<span class="literal">None</span>,</span><br><span class="line">                               use_bias=<span class="literal">False</span>,</span><br><span class="line">                               padding=<span class="string">'same'</span> <span class="keyword">if</span> stride == <span class="number">1</span> <span class="keyword">else</span> <span class="string">'valid'</span>,</span><br><span class="line">                               name=prefix + <span class="string">'depthwise'</span>)(x)</span><br><span class="line">    x = BatchNormalization(epsilon=<span class="number">1e-3</span>,</span><br><span class="line">                                  momentum=<span class="number">0.999</span>,</span><br><span class="line">                                  name=prefix + <span class="string">'depthwise_BN'</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Activation(relu6, name=prefix + <span class="string">'depthwise_relu'</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># part3压缩特征，而且不使用relu函数，保证特征不被破坏</span></span><br><span class="line">    x = Conv2D(pointwise_filters,</span><br><span class="line">                      kernel_size=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="string">'same'</span>,</span><br><span class="line">                      use_bias=<span class="literal">False</span>,</span><br><span class="line">                      activation=<span class="literal">None</span>,</span><br><span class="line">                      name=prefix + <span class="string">'project'</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = BatchNormalization(epsilon=<span class="number">1e-3</span>,</span><br><span class="line">                                  momentum=<span class="number">0.999</span>,</span><br><span class="line">                                  name=prefix + <span class="string">'project_BN'</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> in_channels == pointwise_filters <span class="keyword">and</span> stride == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> Add(name=prefix + <span class="string">'add'</span>)([inputs, x])</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h4 id="简述DeeplabV3"><a href="#简述DeeplabV3" class="headerlink" title="简述DeeplabV3"></a>简述DeeplabV3</h4><p>DeeplabV3+被认为是语义分割的新高峰，主要是因为这个模型的效果非常的好<br>DeepLabv3+主要在模型的架构上作文章，为了融合多尺度信息，其引入了语义分割常用的encoder-decoder形式。在 encoder-decoder 架构中，引入可<strong>任意控制编码器提取特征</strong>的分辨率，通过<strong>空洞卷积</strong>平衡精度和耗时。</p>
<p>空洞卷积是什么？操作如其名，跨过一些点进行卷积，看下图确实挺空的</p>
<p>好处在哪？感受野更大，提取特征更广泛有效</p>
<p><img src="https://img-blog.csdnimg.cn/20191111205857843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<p>不用懵逼😵，来看如下的结构图，也挺懵的，那就分为Encoder-Decoder来说</p>
<p><img src="https://img-blog.csdnimg.cn/20191111203311239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<h4 id="Encoder模块-稍有特殊的MobilenetV2"><a href="#Encoder模块-稍有特殊的MobilenetV2" class="headerlink" title="Encoder模块-稍有特殊的MobilenetV2"></a>Encoder模块-稍有特殊的MobilenetV2</h4><p>首先在文前说明，DeeplabV3中的MobilenetV2最大的不同在于引入了空洞卷积，具体见下文</p>
<p>1、在主干DCNN深度卷积神经网络里使用串行的Atrous Convolution。串行的意思就是一层又一层，普通的深度卷积神经网络的结构就是<strong>串行结构</strong>。<br>2、在图片经过主干DCNN深度卷积神经网络之后的结果分为两部分，一部分直接传入Decoder，另一部分经过<strong>并行的Atrous Convolution</strong>，分别<strong>用不同rate的Atrous Convolution进行特征提取</strong>，再进行合并，再进行1x1卷积压缩特征。</p>
<p>也就是先是正常的卷积叠加，叠加到中间时得到中间层特征就直接传给Decoder了，然后接着卷积，之后重点来了：</p>
<p><strong>采取并行的结构不同膨胀率,不同filter的卷积/池化分别进行特征提取</strong>，有点类似与Inception的核心思想</p>
<p>提取出不同的特征然后拼接起来使用1 x 1 Conv进行特征压缩</p>
<p>先看看Encoder的主干代码：</p>
<p>一目了然：多个反残差卷积块进行</p>
<p>看看输出：return x,skip1</p>
<p>x表示上图串行结构的最终输出，skip1表示中间某层的输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenetV2</span><span class="params">(inputs,alpha=<span class="number">1</span>)</span>:</span></span><br><span class="line">    first_block_filters = _make_divisible(<span class="number">32</span> * alpha, <span class="number">8</span>)</span><br><span class="line">    <span class="comment"># 416,416 -&gt; 208,208</span></span><br><span class="line">    x = Conv2D(first_block_filters,</span><br><span class="line">                kernel_size=<span class="number">3</span>,</span><br><span class="line">                strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>,</span><br><span class="line">                use_bias=<span class="literal">False</span>, name=<span class="string">'Conv'</span>)(inputs)</span><br><span class="line">    x = BatchNormalization(</span><br><span class="line">        epsilon=<span class="number">1e-3</span>, momentum=<span class="number">0.999</span>, name=<span class="string">'Conv_BN'</span>)(x)</span><br><span class="line">    x = Activation(relu6, name=<span class="string">'Conv_Relu6'</span>)(x)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">16</span>, alpha=alpha, stride=<span class="number">1</span>,</span><br><span class="line">                            expansion=<span class="number">1</span>, block_id=<span class="number">0</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 208,208 -&gt; 104,104</span></span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">24</span>, alpha=alpha, stride=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">1</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">24</span>, alpha=alpha, stride=<span class="number">1</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">2</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    skip1 = x</span><br><span class="line">    <span class="comment"># 104,104 -&gt; 52,52</span></span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">32</span>, alpha=alpha, stride=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">3</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">32</span>, alpha=alpha, stride=<span class="number">1</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">4</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">32</span>, alpha=alpha, stride=<span class="number">1</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">5</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#---------------------------------------------------------------#</span></span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">64</span>, alpha=alpha, stride=<span class="number">1</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">6</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">64</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">7</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">64</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">8</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">64</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">9</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">96</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">10</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">96</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">11</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">96</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">12</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">160</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">2</span>,  <span class="comment"># 1!</span></span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">13</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">160</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">4</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">14</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">160</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">4</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">15</span>, skip_connection=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x = _inverted_res_block(x, filters=<span class="number">320</span>, alpha=alpha, stride=<span class="number">1</span>, rate=<span class="number">4</span>,</span><br><span class="line">                            expansion=<span class="number">6</span>, block_id=<span class="number">16</span>, skip_connection=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> x,skip1</span><br></pre></td></tr></table></figure>

<p>但是要注意，这里的反残差卷积块和普通的MobileNet V2有些许不同，我们看看反残差卷积块的实现代码：</p>
<p>参数中有<strong>skip_connecttions</strong>，用于决定是否要有<strong>short_cut的结构</strong></p>
<p>而还有一点更重要的在于<strong>rate=1</strong>，<strong>rate表示空洞卷积的程度</strong>，也就是空的多与少，当然这个空洞卷积是在Depthwise Conv的基础上进行的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_inverted_res_block</span><span class="params">(inputs, expansion, stride, alpha, filters, block_id, skip_connection, rate=<span class="number">1</span>)</span>:</span></span><br><span class="line">    in_channels = inputs.shape[<span class="number">-1</span>].value  <span class="comment"># inputs._keras_shape[-1]</span></span><br><span class="line">    pointwise_conv_filters = int(filters * alpha)</span><br><span class="line">    pointwise_filters = _make_divisible(pointwise_conv_filters, <span class="number">8</span>)</span><br><span class="line">    x = inputs</span><br><span class="line">    prefix = <span class="string">'expanded_conv_&#123;&#125;_'</span>.format(block_id)</span><br><span class="line">    <span class="keyword">if</span> block_id:</span><br><span class="line">        <span class="comment"># Expand</span></span><br><span class="line"></span><br><span class="line">        x = Conv2D(expansion * in_channels, kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>,</span><br><span class="line">                   use_bias=<span class="literal">False</span>, activation=<span class="literal">None</span>,</span><br><span class="line">                   name=prefix + <span class="string">'expand'</span>)(x)</span><br><span class="line">        x = BatchNormalization(epsilon=<span class="number">1e-3</span>, momentum=<span class="number">0.999</span>,</span><br><span class="line">                               name=prefix + <span class="string">'expand_BN'</span>)(x)</span><br><span class="line">        x = Activation(relu6, name=prefix + <span class="string">'expand_relu'</span>)(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prefix = <span class="string">'expanded_conv_'</span></span><br><span class="line">    <span class="comment"># Depthwise</span></span><br><span class="line">    x = DepthwiseConv2D(kernel_size=<span class="number">3</span>, strides=stride, activation=<span class="literal">None</span>,</span><br><span class="line">                        use_bias=<span class="literal">False</span>, padding=<span class="string">'same'</span>, dilation_rate=(rate, rate),</span><br><span class="line">                        name=prefix + <span class="string">'depthwise'</span>)(x)</span><br><span class="line">    x = BatchNormalization(epsilon=<span class="number">1e-3</span>, momentum=<span class="number">0.999</span>,</span><br><span class="line">                           name=prefix + <span class="string">'depthwise_BN'</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Activation(relu6, name=prefix + <span class="string">'depthwise_relu'</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Project</span></span><br><span class="line">    x = Conv2D(pointwise_filters,</span><br><span class="line">               kernel_size=<span class="number">1</span>, padding=<span class="string">'same'</span>, use_bias=<span class="literal">False</span>, activation=<span class="literal">None</span>,</span><br><span class="line">               name=prefix + <span class="string">'project'</span>)(x)</span><br><span class="line">    x = BatchNormalization(epsilon=<span class="number">1e-3</span>, momentum=<span class="number">0.999</span>,</span><br><span class="line">                           name=prefix + <span class="string">'project_BN'</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> skip_connection:</span><br><span class="line">        <span class="keyword">return</span> Add(name=prefix + <span class="string">'add'</span>)([inputs, x])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if in_channels == pointwise_filters and stride == 1:</span></span><br><span class="line">    <span class="comment">#    return Add(name='res_connect_' + str(block_id))([inputs, x])</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>接着并行的结构在哪里体现呢？我们直接进入DeeplabV3进行查看，见如下代码</p>
<ol>
<li>并行结构</li>
</ol>
<ul>
<li>在11-25行也就是第一种方式：全局平均池化</li>
<li>操作为先全局平均扩充维度使shape为1 x 1 x 320后使用conv压缩通道，再直接resize扩大分辨率，在进行conv，输出为52 x 52 x 256</li>
<li>第27-29使第二种方式：直接卷积调整通道，输出为52 x 52 x 256</li>
<li>第33行为第三种方式：SepConv_BN，有一个rate值为膨胀率，也就是先3x3膨胀卷积扩大通道数，再1x1卷积收缩通道数为256，进行压缩，输出为52 x 52 x 256  </li>
<li>其余两种类似，只不过膨胀率不同</li>
</ul>
<ol start="2">
<li><p>拼接：最后将5中进行拼接，输出为 52 x 52 x 256*5</p>
</li>
<li><p>卷积改变通道数：输出为 52 x 52 x 256</p>
</li>
<li><p>调整分辨率与中间层的输出对应，中间层分辨率为104 x 104，则此时52 x 52 x 256——104 x 104 x 256</p>
</li>
<li><p>卷积对skip1调整通道数，输出为104 x 104 x 48</p>
</li>
<li><p>第62行，拼接，输出为104 x 104 x 304（256+48）</p>
</li>
<li><p>第69行</p>
</li>
</ol>
<ul>
<li>首先获取输入图片的大小 416 x 416 x 21</li>
<li>接着卷积改变拼接后图像的通道数为类别数——104 x 104 x 21</li>
<li>接着改变拼接后图像的分辨率 ——416 x 416 x 21</li>
<li>softmax分类</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Deeplabv3</span><span class="params">(input_shape=<span class="params">(<span class="number">416</span>, <span class="number">416</span>, <span class="number">3</span>)</span>, classes=<span class="number">21</span>, alpha=<span class="number">1.</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    img_input = Input(shape=input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (52, 52, 320)</span></span><br><span class="line">    x,skip1 = mobilenetV2(img_input,alpha)</span><br><span class="line">    size_before = tf.keras.backend.int_shape(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全部求平均后，再利用expand_dims扩充维度，1x1</span></span><br><span class="line">    <span class="comment"># shape = 320</span></span><br><span class="line">    b4 = GlobalAveragePooling2D()(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1x1x320</span></span><br><span class="line">    b4 = Lambda(<span class="keyword">lambda</span> x: K.expand_dims(x, <span class="number">1</span>))(b4)</span><br><span class="line">    b4 = Lambda(<span class="keyword">lambda</span> x: K.expand_dims(x, <span class="number">1</span>))(b4)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 压缩filter</span></span><br><span class="line">    b4 = Conv2D(<span class="number">256</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>,</span><br><span class="line">                use_bias=<span class="literal">False</span>, name=<span class="string">'image_pooling'</span>)(b4)</span><br><span class="line">    b4 = BatchNormalization(name=<span class="string">'image_pooling_BN'</span>, epsilon=<span class="number">1e-5</span>)(b4)</span><br><span class="line">    b4 = Activation(<span class="string">'relu'</span>)(b4)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接利用resize_images扩充hw</span></span><br><span class="line">    <span class="comment"># b4 = 52,52,256</span></span><br><span class="line">    b4 = Lambda(<span class="keyword">lambda</span> x: tf.image.resize_images(x, size_before[<span class="number">1</span>:<span class="number">3</span>]))(b4)</span><br><span class="line">    <span class="comment"># 调整通道</span></span><br><span class="line">    b0 = Conv2D(<span class="number">256</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>, use_bias=<span class="literal">False</span>, name=<span class="string">'aspp0'</span>)(x)</span><br><span class="line">    b0 = BatchNormalization(name=<span class="string">'aspp0_BN'</span>, epsilon=<span class="number">1e-5</span>)(b0)</span><br><span class="line">    b0 = Activation(<span class="string">'relu'</span>, name=<span class="string">'aspp0_activation'</span>)(b0)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># rate值与OS相关，SepConv_BN为先3x3膨胀卷积，再1x1卷积，进行压缩</span></span><br><span class="line">    <span class="comment"># 其膨胀率就是rate值</span></span><br><span class="line">    b1 = SepConv_BN(x, <span class="number">256</span>, <span class="string">'aspp1'</span>,</span><br><span class="line">                    rate=<span class="number">6</span>, depth_activation=<span class="literal">True</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">    b2 = SepConv_BN(x, <span class="number">256</span>, <span class="string">'aspp2'</span>,</span><br><span class="line">                    rate=<span class="number">12</span>, depth_activation=<span class="literal">True</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">    b3 = SepConv_BN(x, <span class="number">256</span>, <span class="string">'aspp3'</span>,</span><br><span class="line">                    rate=<span class="number">18</span>, depth_activation=<span class="literal">True</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    x = Concatenate()([b4, b0, b1, b2, b3])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用conv2d压缩</span></span><br><span class="line">    <span class="comment"># 52,52,256</span></span><br><span class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>,</span><br><span class="line">               use_bias=<span class="literal">False</span>, name=<span class="string">'concat_projection'</span>)(x)</span><br><span class="line">    x = BatchNormalization(name=<span class="string">'concat_projection_BN'</span>, epsilon=<span class="number">1e-5</span>)(x)</span><br><span class="line">    x = Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line">    x = Dropout(<span class="number">0.1</span>)(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># skip1.shape[1:3] 为 104,104</span></span><br><span class="line">    <span class="comment"># skip1 104, 104, 256</span></span><br><span class="line">    x = Lambda(<span class="keyword">lambda</span> xx: tf.image.resize_images(x, skip1.shape[<span class="number">1</span>:<span class="number">3</span>]))(x)</span><br><span class="line">                                                    </span><br><span class="line">    <span class="comment"># 104, 104, 48</span></span><br><span class="line">    dec_skip1 = Conv2D(<span class="number">48</span>, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>,</span><br><span class="line">                        use_bias=<span class="literal">False</span>, name=<span class="string">'feature_projection0'</span>)(skip1)</span><br><span class="line">    dec_skip1 = BatchNormalization(</span><br><span class="line">        name=<span class="string">'feature_projection0_BN'</span>, epsilon=<span class="number">1e-5</span>)(dec_skip1)</span><br><span class="line">    dec_skip1 = Activation(<span class="string">'relu'</span>)(dec_skip1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 104,104,304</span></span><br><span class="line">    x = Concatenate()([x, dec_skip1])</span><br><span class="line">    x = SepConv_BN(x, <span class="number">256</span>, <span class="string">'decoder_conv0'</span>,</span><br><span class="line">                    depth_activation=<span class="literal">True</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">    x = SepConv_BN(x, <span class="number">256</span>, <span class="string">'decoder_conv1'</span>,</span><br><span class="line">                    depth_activation=<span class="literal">True</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 416,416,2</span></span><br><span class="line">    size_before3 = tf.keras.backend.int_shape(img_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 52,52,2</span></span><br><span class="line">    x = Conv2D(classes, (<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>)(x)</span><br><span class="line">    x = Lambda(<span class="keyword">lambda</span> xx:tf.image.resize_images(xx,size_before3[<span class="number">1</span>:<span class="number">3</span>]))(x)</span><br><span class="line"></span><br><span class="line">    x = Reshape((<span class="number">-1</span>,classes))(x)</span><br><span class="line">    x = Softmax()(x)</span><br><span class="line"></span><br><span class="line">    inputs = img_input</span><br><span class="line">    model = Model(inputs, x, name=<span class="string">'deeplabv3plus'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h4 id="Decoder模块-3"><a href="#Decoder模块-3" class="headerlink" title="Decoder模块"></a>Decoder模块</h4><p>看到这里，你会发现Decoder部分也就是上边代码的第40行到最后</p>
<p>上边的序号流程也讲的比较清楚，这里不再赘述</p>
<h4 id="Deeplab-V3小结"><a href="#Deeplab-V3小结" class="headerlink" title="Deeplab V3小结"></a>Deeplab V3小结</h4><p>首先是Encoder模块中的Mobilenet V2，<strong>将Resnet 50的结构反过来，先升维，然后进行的不是普通的3 x 3 Conv，而是Depthwise Conv，再1 x 1降维，降维后没有使用Relu，最后进行拼接起来</strong>，并且其中的Depthwise Conv加入了<strong>空洞卷积</strong></p>
<p>其次是输出时有两个输出，一个是skip1表示中间层输出，二个是船形结构的最终输出</p>
<p>最后是核心，也就是采取多种膨胀率的卷积的并行结构，进行拼接，再调整后与skip1拼接，分类得到最终的结果</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h4 id="数据集与标签的形式"><a href="#数据集与标签的形式" class="headerlink" title="数据集与标签的形式"></a>数据集与标签的形式</h4><p>在图像分类中，一张图片对应一个标签，比如224 x 224 分辨率大小的图片，在5分类任务中如果它是第一类，那么对应的标签为[0,0,0,0,1]</p>
<p>在语义分割任务中，由于需要对像素点打标签，又有所不同了，打标签的难度也会提高。选取21分类的VOC数据集中的一张图进行展示</p>
<p>原图：</p>
<p><img src="https://img-blog.csdnimg.cn/20191108211712526.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<p>所打标签</p>
<p><img src="https://img-blog.csdnimg.cn/20191108211726987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<p>比如上图原图调整为416 x 416分辨率，并进行img/255归一化</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">print</span><span class="params">(img[:,:,<span class="number">0</span>])</span></span></span><br></pre></td></tr></table></figure>

<p>[[ 0.92156863  0.92156863  0.92156863 …,  0.08627451  0.10196078<br>   0.10980392]<br> [ 0.91372549  0.91372549  0.91372549 …,  0.08235294  0.09411765<br>   0.10196078]<br> [ 0.91372549  0.91372549  0.91372549 …,  0.10980392  0.11764706<br>   0.12156863]<br> …,<br> [ 0.09411765  0.09411765  0.09803922 …,  0.0745098   0.0745098<br>   0.0745098 ]<br> [ 0.13333333  0.13333333  0.1372549  …,  0.07058824  0.07058824<br>   0.07058824]<br> [ 0.15686275  0.15686275  0.16078431 …,  0.08235294  0.08235294<br>   0.08235294]]</p>
<p>我们再看看标签图长什么样</p>
<p>先看看各个维度的数据是否相同，<code>print((img[:,:,0]==img[:,:,2]) == (img[:,:,0] == img[:,:,1]))</code>,从下图可以看出都是一样的</p>
<p><img src="https://upload-images.jianshu.io/upload_images/13652833-16e1889166a65031.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随便打印一个维度看一看 <code>print(img[:,:,0])</code></p>
<p>从下面结果可以看出，标签图对应的只有0和19，19对应的是火车Train的类别，0对应的是背景的类别</p>
<p> 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19<br>   19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19<br>   19  19]<br> [  0  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19<br>   19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19<br>   19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19<br>   19  19  19  19  19  19  19  19  19  19  19  19   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0<br>    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0</p>
<p>所以，<strong>在训练集中，如果像本文一样分两类，那么背景的RGB就是000，斑马线的RGB就是111，如果分多类，那么还会存在222，333，444这样的。这说明其属于不同的类。</strong></p>
<h4 id="损失函数loss-准备预测值与真实值"><a href="#损失函数loss-准备预测值与真实值" class="headerlink" title="损失函数loss-准备预测值与真实值"></a>损失函数loss-准备预测值与真实值</h4><p>loss通常有真实值和预测值进行交叉熵</p>
<p>对于图像分类，每张图片得到nclassesx1的预测值，然后与标签依然为nclassesx1的形式的真实值，直接进行交叉熵计算loss即可</p>
<p>对于语义分割，我们以Pspnet为例，先来看看输出的预测值，<strong>输出的shape为144*144 x n_classes</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">o = Conv2D( n_classes,(<span class="number">3</span>,<span class="number">3</span>),data_format=IMAGE_ORDERING, padding=<span class="string">'same'</span> )(o)</span><br><span class="line">o = resize_image(o,(<span class="number">8</span>,<span class="number">8</span>),data_format=IMAGE_ORDERING)</span><br><span class="line">o = Reshape((<span class="number">-1</span>,n_classes))(o)</span><br><span class="line">o = Softmax()(o)</span><br><span class="line">model = Model(img_input,o)</span><br></pre></td></tr></table></figure>
<p>然后是真实值，也就是标签图像，看看是如何处理的</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">img = Image.open(<span class="string">r".\dataset2\png"</span> + <span class="string">'/'</span> + name)</span><br><span class="line">img = img.resize((int(WIDTH<span class="regexp">/4),int(HEIGHT/</span><span class="number">4</span>)))</span><br><span class="line">img = np.array(img)</span><br><span class="line">seg_labels = np.zeros((int(HEIGHT<span class="regexp">/4),int(WIDTH/</span><span class="number">4</span>),NCLASSES))</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(NCLASSES):</span><br><span class="line">    seg_labels[: , : , c ] = (img[:,:,<span class="number">0</span>] == c ).astype(int)</span><br><span class="line">seg_labels = np.reshape(seg_labels, (-<span class="number">1</span>,NCLASSES))</span><br><span class="line">Y_train.append(seg_labels)</span><br></pre></td></tr></table></figure>

<p>在第二行，缩小4倍，也就是<strong>分辨率为144 x 144</strong>（pspnet输入图像分辨率为576 x 576）</p>
<p>新建一个二seg_labels，shape同为144 x 144 x n_classes</p>
<p>在第5行for循环中，对于每一个类，比如对于火车Train类19，操作是这样的</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">seg_labels[: , : , <span class="number">19</span> ] = (img[:,:,<span class="number">0</span>] == <span class="number">19</span> ).astype(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure>

<p>我们打印出来其中的一部分看看是怎样的（一部分方便对比展示）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = Image.open(<span class="string">r".\2.jpg"</span>)</span><br><span class="line">img = img.resize((int(WIDTH/<span class="number">4</span>),int(HEIGHT/<span class="number">4</span>)))</span><br><span class="line">img = np.array(img)</span><br><span class="line">print(img[:,:,<span class="number">0</span>][<span class="number">50</span>:<span class="number">52</span>])</span><br><span class="line">print(<span class="string">"*"</span>*<span class="number">20</span>)</span><br><span class="line">print(<span class="string">"*"</span>*<span class="number">20</span>)</span><br><span class="line">seg_labels = np.zeros((int(HEIGHT/<span class="number">4</span>),int(WIDTH/<span class="number">4</span>),NCLASSES))</span><br><span class="line">print((img[:,:,<span class="number">0</span>]==<span class="number">19</span>)[<span class="number">50</span>:<span class="number">52</span>])</span><br></pre></td></tr></table></figure>

<p>结果如下图，我们可以发现为19的像素点自动被更改为了True</p>
<p><img src="https://upload-images.jianshu.io/upload_images/13652833-1a832eca0e5d8ab9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么这个shape为144 x 144 x nclasses的seg_labels的结果就可想而知了，也就是对于每一个通道（n_classes）对于每一个类，有一个144 x 144的矩阵，其中每一个点值如果<strong>为1表示这个像素点为此类，如果为0表示这个像素点非此类，为其他类</strong>，其实仔细一想，和图像分类中的one-hot不一样的吗？[1,0,0,0]为1表示是第1个类别，为0表示是其他类别</p>
<p>之后reshape为144*144 x nclasses的状态，与预测值shape相同，便可以调用交叉熵进行像素级别的loss计算了</p>
<h4 id="损失函数loss-交叉熵计算"><a href="#损失函数loss-交叉熵计算" class="headerlink" title="损失函数loss-交叉熵计算"></a>损失函数loss-交叉熵计算</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(y_true, y_pred)</span>:</span>    </span><br><span class="line">	crossloss = K.binary_crossentropy(y_true,y_pred)    </span><br><span class="line">	loss = <span class="number">16</span> * K.sum(crossloss)/HEIGHT/WIDTH    </span><br><span class="line">	<span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<p>调用二分类形式的交叉熵函数计算即可</p>
<h4 id="其余"><a href="#其余" class="headerlink" title="其余"></a>其余</h4><p>其余关于预测准确率以及早停、保存模型等就不赘述了</p>
]]></content>
      <categories>
        <category>滴滴实习-语义分割</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
        <tag>Deeplab</tag>
        <tag>Segnet</tag>
        <tag>Unet</tag>
        <tag>Pspnet</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo基本操作</title>
    <url>/2020/03/10/hexo%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="Hexo添加menu中的分类-标签等"><a href="#Hexo添加menu中的分类-标签等" class="headerlink" title="Hexo添加menu中的分类/标签等"></a>Hexo添加menu中的分类/标签等</h2><p>博客最基本的需求就是对博文进行分类，初始化的Hexo只有首页和归档，如何进行添加呢？</p>
<ol>
<li>新建<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">hexo new<span class="built_in"> page </span>categories</span><br></pre></td></tr></table></figure>
这样会在/Hexo/source新建categories文件夹，里边包含index.md</li>
<li>处理index.md<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">categories</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2017</span><span class="number">-12</span><span class="number">-02</span> <span class="number">21</span><span class="string">:01:24</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"categories"</span></span><br></pre></td></tr></table></figure>
在其中加入type字段即可</li>
</ol>
<a id="more"></a>

<ol start="3">
<li><p>处理主题配置文件</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">menu:</span></span><br><span class="line"><span class="symbol">  home:</span> / || home</span><br><span class="line">  <span class="meta">#about: /about/ || user</span></span><br><span class="line"><span class="symbol">  tags:</span> <span class="meta-keyword">/tags/</span> || tags</span><br><span class="line"><span class="symbol">  categories:</span> <span class="meta-keyword">/categories/</span> || th</span><br><span class="line"><span class="symbol">  archives:</span> <span class="meta-keyword">/archives/</span> || archive</span><br><span class="line">  <span class="meta">#schedule: /schedule/ || calendar</span></span><br><span class="line">  <span class="meta">#sitemap: /sitemap.xml || sitemap</span></span><br><span class="line">  <span class="meta">#commonweal: /404/ || heartbeat</span></span><br></pre></td></tr></table></figure>
<p>将其中的注释删除</p>
</li>
<li><p>部署</p>
<p> <code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p>
</li>
</ol>
<h2 id="发布博文"><a href="#发布博文" class="headerlink" title="发布博文"></a>发布博文</h2><ol>
<li><p>git bash新建<br><code>hexo new &quot;文章名称&quot;</code><br>\Hexo\source_posts会出现相关的文章名称.md文件</p>
</li>
<li><p>修改.md文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">title</span> <span class="comment">#文章標題</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2016</span><span class="number">-06</span><span class="number">-01</span> <span class="number">23</span><span class="string">:47:44</span> <span class="comment">#文章生成時間</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">"Hexo教程"</span> <span class="comment">#文章分類目錄 可以省略</span></span><br><span class="line"><span class="attr">tags:</span> <span class="comment">#文章標籤 可以省略</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">标签1</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">标签2</span></span><br><span class="line"> <span class="attr">description:</span> <span class="comment">#你對本頁的描述 可以省略</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>​    初始化只有title和data，添加categories和tags即可</p>
<ol start="3">
<li>撰写/保存/部署<br>其中不要忘记加入<code>&lt;!-- more --&gt;</code>进行摘要展示</li>
</ol>
<h2 id="常用的MD语法"><a href="#常用的MD语法" class="headerlink" title="常用的MD语法"></a>常用的MD语法</h2><p>​    MD用的少，word与latex比较熟悉，所以记录一下常用的</p>
<h4 id="标记与多重标记"><a href="#标记与多重标记" class="headerlink" title="标记与多重标记"></a>标记与多重标记</h4><p>使用&gt;与&gt;&gt;表示标记</p>
<blockquote>
<p>标记里再使用 </p>
<blockquote>
<p>标记</p>
</blockquote>
</blockquote>
<h4 id="多级标题"><a href="#多级标题" class="headerlink" title="多级标题"></a>多级标题</h4><p>使用1-6个#表示多级标题</p>
<h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><p>+-*表示无序列表</p>
<ul>
<li>我的微信公众号</li>
<li>我的微信公众号</li>
<li>我的微信公众号</li>
</ul>
<ul>
<li>我的尾巴</li>
</ul>
<p>数字加英文.表示有序列表</p>
<h4 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h4><p>分割线可以由* - _（星号，减号，底线）这3个符号的至少3个符号表示，注意至少要3个，且不需要连续，有空格也可以</p>
<hr>
<h4 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h4><p>链接的文字放在[]中，链接地址放在随后的（）中<br><a href="https://www.zhihu.com/people/liu-hao-33-54" target="_blank" rel="noopener">我的知乎</a></p>
<h4 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h4><p>与链接基本一致，加入！即可<br><img src="https://raw.githubusercontent.com/smshen/MarkdownPhotos/master/Res/test.jpg" alt="微信"></p>
<p>但是如果为本地图片地址，采取如下的方式：</p>
<h6 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h6><ul>
<li>在配置文件<strong>_config.yml</strong>里修改：<code>post_asset_folder: true</code></li>
<li>在Hexo安装目录下执行:<code>npm install hexo-asset-image --save</code>，这是下载安装一个可以上传本地图片的插件</li>
<li>等待一段时间之后，再运行<code>hexo n &quot;文章标题&quot;</code>来生成博文时，<code>/source/_post</code>文件夹中除了<code>文章标题.md</code>外，还有一个同名文件夹。</li>
<li>在新的博文中想引入图片时，可以先把图片复制到博文的同名文件夹，然后在<code>.md</code>中按照常规的方式饮用图片即可，如<code>![你想输入的替代文字](博文标题/图片名.jpg)</code>。<strong>注意，此处的图片路径必须使用相对路径</strong></li>
<li>执行<code>hexo g</code>,检查生成的页面中图片的src地址。此时生成页面中图片src地址应该与页面的相对路径一致（具体路径取决于页面路径格式设置）</li>
</ul>
<h6 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h6><p>以上方法可以解决本地图片上传和引用的问题，但是在每个文章下建立资源文件夹好处是分类清楚，缺点是图片复用不方便，也不符合网站设计的一般规范。</p>
<p>所以我们可以第二种方案：</p>
<ul>
<li>在本地source中建立img文件夹，将引用到的图片全部放在此文件夹中。这样操作也便于图片的复用。</li>
<li><strong>注意，采用这种方法时无需修改_config.yml,也无需安装hexo-asset-image</strong></li>
</ul>
<h4 id="强调"><a href="#强调" class="headerlink" title="强调"></a>强调</h4><p>*倾斜*<br>**加粗**<br>~~删除~~<br><em>倾斜</em><br><strong>加粗</strong><br><del>删除</del></p>
<h4 id="公式-转义"><a href="#公式-转义" class="headerlink" title="公式/转义"></a>公式/转义</h4><p>转移和latex等都差不多，用\即可</p>
<p>公式：</p>
<p>在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。</p>
<h6 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h6><p>Hexo 默认使用 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线<code>_</code>代表斜体，会被渲染引擎处理为``标签。</p>
<p>因为类 Latex 格式书写的数学公式下划线<code>_</code>表示下标，有特殊的含义，如果被强制转换为``标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。</p>
<p>类似的语义冲突的符号还包括<code>*</code>, <code>{</code>, <code>}</code>, <code>\\</code>等。 </p>
<h6 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h6><p>更换 Hexo 的 markdown 渲染引擎，hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-renderer-marked <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-kramed <span class="comment">--save1212</span></span><br></pre></td></tr></table></figure>

<p>执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。<br>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为 hexo-renderer-kramed 引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的 escape 变量的值做相应的修改：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">//escape: /^\\([<span class="string">\\`*&#123;&#125;\[\</span>](<span class="link"></span>)#$+\-.!_&gt;])/,</span><br><span class="line">escape: /^\\([<span class="string">`*\[\</span>](<span class="link"></span>)#$+\-.!_&gt;])/,1212</span><br></pre></td></tr></table></figure>

<p>这一步是在原基础上取消了对,{,}的转义(escape)。<br>同时把第20行的em变量也要做相应的修改。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">//em: /^\b<span class="emphasis">_((?:_</span><span class="emphasis">_|[\s\S])+?)_</span>\b|^\<span class="emphasis">*((?:\*</span>\<span class="emphasis">*|[\s\S])+?)\*</span>(?!\*)/,</span><br><span class="line">em: /^\<span class="emphasis">*((?:\*</span>\<span class="emphasis">*|[\s\S])+?)\*</span>(?!\*)/,1212</span><br></pre></td></tr></table></figure>

<p>重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。</p>
<h6 id="在-Next-主题中开启-MathJax-开关"><a href="#在-Next-主题中开启-MathJax-开关" class="headerlink" title="在 Next 主题中开启 MathJax 开关"></a>在 Next 主题中开启 MathJax 开关</h6><p>如何使用了主题了，别忘了在主题（Theme）中开启 MathJax 开关，下面以 next 主题为例，介绍下如何打开 MathJax 开关。</p>
<p>进入到主题目录，找到 _config.yml 配置问题，把 math 默认的 false 修改为true，具体如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Math Equations Render Support</span></span><br><span class="line"><span class="attr">math:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default(true) will load mathjax/katex script on demand</span></span><br><span class="line">  <span class="comment"># That is it only render those page who has 'mathjax: true' in Front Matter.</span></span><br><span class="line">  <span class="comment"># If you set it to false, it will load mathjax/katex srcipt EVERY PAGE.</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">engine:</span> <span class="string">mathjax</span></span><br><span class="line">  <span class="comment">#engine: katex12345678910111234567891011</span></span><br></pre></td></tr></table></figure>

<p>还需要在文章的Front-matter里打开mathjax开关，如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">index.html</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="number">-07</span><span class="number">-05</span> <span class="number">12</span><span class="string">:01:30</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">--123456123456</span></span><br></pre></td></tr></table></figure>

<p>之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>如果代码量比较少，只有单行的话，可以用单反引号包起来，如下：<br><code>asd</code><br>如果多行就用</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> main&#123;</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; temp;</span><br><span class="line">	<span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Hexo操作</category>
      </categories>
      <tags>
        <tag>-hexo -md</tag>
      </tags>
  </entry>
  <entry>
    <title>关于博客</title>
    <url>/2020/03/08/%E5%85%B3%E4%BA%8E%E8%BF%99%E4%B8%AA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>为了规则、规律地进行记录与分享,主要分为以下4个方面</p>
<h2 id="c-刷题笔记"><a href="#c-刷题笔记" class="headerlink" title="c++/刷题笔记"></a>c++/刷题笔记</h2><h2 id="时事吐槽"><a href="#时事吐槽" class="headerlink" title="时事吐槽"></a>时事吐槽</h2><h2 id="学习-实习"><a href="#学习-实习" class="headerlink" title="学习/实习"></a>学习/实习</h2><h2 id="comment-share"><a href="#comment-share" class="headerlink" title="comment/share"></a>comment/share</h2>]]></content>
  </entry>
</search>
